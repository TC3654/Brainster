{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24d87f5-ff0c-4847-978c-2985e1ce4193",
   "metadata": {},
   "source": [
    "# Week 8: Parameter Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b14e0c-f813-4df6-baab-33f1f894dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the libraries\n",
    "import numpy as np\n",
    "#import sympy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as opt\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec7758d-9607-455b-b23e-66a6c142c9d8",
   "metadata": {},
   "source": [
    "## Day 3: Bayesian Parameter Estimation\n",
    "\n",
    "We have already used the Maximum Likelihood method to estimate the value of a distribution parameter. To do this, we made an assumption about the shape of the distribution and obtained some data. One thing we did not do is make any assumptions about the parameters themselves. In other words, we were ignorant about the possible range of values that the parameter might *prefer* (except for explicit mathematical limitations).\n",
    "\n",
    "The Bayesian Parameter Estimation is a process which, as the name suggests, uses the Bayes' theorem to estimate parameters. However, unlike the maximum likelihood approach, now we have to make an assumption about what is the **distribution of the parameter** we estimate, before we even start the estimation. The goal is to combine this information with the data to obtain a new distribution for the values of the parameter *after* we have obtained the data.\n",
    "\n",
    "## How does it work?\n",
    "Let $X$ be a random variable with parameter $\\theta \\in \\Theta$ where $\\Theta$ is the set of possible values for $\\theta$.\n",
    "\n",
    "First, we assume that $\\theta$ has a certain **prior** distribution in $\\Theta$. Let's denote it by $p_\\Theta (\\theta)$\n",
    "\n",
    "Next, collect some data for $X$. Since $\\theta$ is a parameter of this random variable, the **likelihood** that $X=x$ is given through the density we denote as $p_{X\\mid\\Theta} (x \\mid \\theta)$\n",
    "\n",
    "Now comes the key part: how do we incorporate the information from the prior about $\\theta$ and the likelihood about $x$ in order to gain a better understanding about the values of $\\theta$? In simple words: how does **evidence** (the data, $x$s) influence our understanding of the **assumption** (about $\\theta$ given through the prior)?\n",
    "\n",
    "In a nutshell, the answer lies in the Bayes' theorem. The phrase *evidence influences assumption/prior* is nothing else but the density $p_{\\Theta \\mid X} (\\theta \\mid x)$, which is called **posterior** distribution of the values of the parameter $\\theta$, after some evidence has been collected through the value(s) $x$. Mathematically, the prior, the likelihood and the posterior are related as:\n",
    "$$\n",
    "p_{\\Theta \\mid X} \\left( \\theta \\mid x \\right) = \\frac{p_{X \\mid \\Theta} \\left( x \\mid \\theta \\right) \\cdot p_\\Theta (\\theta) }{p_X (x)}\n",
    "$$\n",
    "The denominator $p_X (x)$, which in practice may be computationally intensive to calculate, is given by:\n",
    "* If $\\theta$ is a continuous random variable\n",
    "$$\n",
    "p_X(x) = \\int_{\\theta_\\min}^{\\theta_\\max} {p_{X \\mid \\Theta} \\left( x \\mid \\theta \\right) \\cdot p_\\Theta (\\theta)}\\, d\\theta\n",
    "$$\n",
    "* If $\\theta$ is a discrete random variable\n",
    "$$\n",
    "p_X(x) = \\sum_{\\text{all } \\theta} {p_{X \\mid \\Theta} \\left( x \\mid \\theta \\right) \\cdot p_\\Theta (\\theta)}\n",
    "$$\n",
    "\n",
    "Once we construct the posterior distribution we can also construct a point-estimate of the value of the parameter called **Maximum A Posteriori** or **MAP** estimate as the mean of the posterior. Further, we can construct confidence intervals to get an interval estimate as well (here bootstrapping is the best option)\n",
    "\n",
    "## Example 1\n",
    "It is believed that cross-fertilized plants produce taller offspring than the self-fertilized plants. In order to obtain an estimate on the proportion $\\theta$ of cross-fertilized plants that are taller, a researcher observes a random sample of $n=15$ pairs of plants that are exactly the same age. Each pair is grown in the same conditions with some cross-fertilized and the others self-fertilized.\n",
    "\n",
    "Based on previous experience, the experimenter believes that the following are possible values of $\\theta$ and that the prior distribution for each value of $\\theta$ given by is $p_\\Theta (\\theta)$\n",
    "\n",
    "| $\\theta$            | 0.80 | 0.82 | 0.84 | 0.86 | 0.88 | 0.90 |\n",
    "|--------------------:|------|------|------|------|------|------|\n",
    "| $p_\\Theta (\\theta)$ | 0.08 | 0.17 | 0.25 | 0.25 | 0.17 | 0.08 |\n",
    "\n",
    "From the experiment, it is observed that in 13 of the 15 pairs, cross-fertilized is taller. Find the posterior distribution of $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd2ab7-ad37-4f13-83c3-51e9aceb9610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given stuff:\n",
    "t = np.array([0.80, 0.82, 0.84, 0.86, 0.88, 0.90])\n",
    "p_t = np.array([0.08, 0.17, 0.25, 0.25, 0.17, 0.08])\n",
    "n = 15\n",
    "k = 13\n",
    "\n",
    "# X - number of cross-fertilized plans that are taller in a group of 15\n",
    "# X ~ B(n, theta)\n",
    "\n",
    "# Build the dataframe\n",
    "df = pd.DataFrame(columns=['theta', 'prior', 'lik', 'lik*prior', 'post'])\n",
    "df['theta'] = t\n",
    "df['prior'] = p_t\n",
    "df['lik'] = \n",
    "df['lik*prior'] = \n",
    "df['post'] = \n",
    "\n",
    "display(df)\n",
    "\n",
    "#Visulaize the prior and the posterior\n",
    "ticks = [str(t) for t in df['theta']]\n",
    "plt.figure()\n",
    "plt.bar(..., label='Prior', alpha=0.5)\n",
    "plt.bar(..., label='Posterior', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c3a06-2abd-4f08-930b-75755cd3b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the prior and the posterior mean\n",
    "prior_mean = ...\n",
    "post_mean = ...\n",
    "\n",
    "print('Prior mean = ', prior_mean)\n",
    "print('Posterior mean = ', post_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff24250-c0fb-499b-a30a-c1ce6b8f5b17",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "A certain random process produces outcomes according to a Poisson distribution. If $X$ is the random variable that models the outcomes, then $X \\sim Po(\\lambda)$. The value of the parameter $\\lambda$ is thought to be uniformly distributed in the interval $(0, 5]$.\n",
    "* Upon observation, we obtain an outcome $X = 4$. Construct the posterior distribution for the outcomes.\n",
    "* Then a second observation is made, $X = 2$. Find the posterior after the two observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f7722-69da-4620-a1e7-eddb104afa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior is lambda ~ U(0, 5]), likelihood is X ~ Po(lambda)\n",
    "def prior(x):\n",
    "    return ...\n",
    "\n",
    "# posterior is likelihood * prior\n",
    "def posterior(lmbd, data):\n",
    "    numerator = lambda x, data: ...\n",
    "    denominator = ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef608314-6cbf-47d6-a70c-150c7ba0a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prior and the posterior to see how things have changed\n",
    "xs = np.linspace(0, 5, 100)\n",
    "ys = ... # prior\n",
    "zs = ... # posterior\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Posterior after the first observation')\n",
    "plt.plot(xs, ys, label='prior')\n",
    "plt.plot(xs, zs, label='posterior')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db1ded-e443-4eee-bded-201f923d20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two observations\n",
    "obs = [4, 2]\n",
    "\n",
    "# The posterior now is: posterior(x, obs)\n",
    "ws = ...\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Posterior after the first observation')\n",
    "plt.plot(xs, ys, label='prior')\n",
    "plt.plot(xs, zs, label='posterior')\n",
    "plt.plot(xs, ws, label='second posterior')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df3c26-a21d-4f82-ad0d-3302c3aabaf5",
   "metadata": {},
   "source": [
    "## Example 2a\n",
    "For the second posterior you constructed in **Example 2**, find the maximum a posteriori estimate (**MAP**) $\\hat\\lambda$ of the parameter $\\lambda$, and using bootstrap construct a 95% confidence interval for it.\n",
    "\n",
    "**Note:** the MAP is the mean/expected value of the posterior distribution for $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3eb2f-8f6a-4c18-8750-fa2826b530fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the mean of the posterior\n",
    "def MAP(x, data):\n",
    "    return \n",
    "\n",
    "lmbd_hat = \n",
    "\n",
    "print('MAP = ', lmbd_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafbaefa-7ed2-417c-be08-3704b072430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the interval\n",
    "# Choose a random sample from the posterior\n",
    "n = 100 # the sample size\n",
    "sample = np.zeros(n) #the empty sample\n",
    "i = 0 # the counter\n",
    "\n",
    "np.random.seed(123)\n",
    "while i < n:\n",
    "    # generate a random point in [0, 5] x [0, 0.5]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f460268-34ac-461e-aa70-9352f015b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check visually\n",
    "plt.figure()\n",
    "plt.hist(sample, density=True, edgecolor = 'k', label='the sample')\n",
    "plt.plot(xs, ws, label='the posterior', c='r')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f25eeb-88df-4820-8d84-488f7bc723b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the interval by bootstrapping\n",
    "m = 1000 # of bootstraps\n",
    "deltas = np.zeros(m)\n",
    "sample_mean = np.mean(sample)\n",
    "\n",
    "np.random.seed(999)\n",
    "for i in range(m):\n",
    "    \n",
    "\n",
    "    \n",
    "# constructing the interval\n",
    "\n",
    "\n",
    "print(f'The 95% \"confidence\" interval is ({}, {})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fdae3-12c9-4a2f-876b-84ae6bfede8d",
   "metadata": {},
   "source": [
    "## Example 3:\n",
    "In a study done at the *National Institute of Science and Technology* in 1980, asbestos fibers on filters were counted as part of a project to develop measurement standards for asbestos concentration. Asbestos dissolved in water was spread on a filter, and 3-mm diameter punches were taken from the filter and mounted on a transmission electron microscope. An operator counted the number of fibers in each of 23 grid squares; the results are given in the cell below.\n",
    "\n",
    "Let $X$ be the random variable that counts how many fibers there are in one grid square. Then $X \\sim Po(\\lambda)$ where $\\lambda$ represents the mean number of fibers per square. Assuming that the prior distribution of $\\lambda$ is a **Gamma distribution** with parameters $\\alpha = 15$ and $\\theta = 1.2$, construct the posterior distribution for $\\lambda$ given the observed experimental data. Find the MAP estimate $\\hat\\lambda$ and the 90% bootstrapped confidence interval for $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3475f181-fe19-49f3-8e7a-d0ac8b99bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The asbestos counts\n",
    "asbestos = np.array([31, 29, 19, 18, 31, 28,\n",
    "                    34, 27, 34, 30, 16, 18,\n",
    "                    26, 27, 27, 18, 24, 22,\n",
    "                    28, 24, 21, 17, 24])\n",
    "\n",
    "#constructing the prior\n",
    "alpha = 15.0\n",
    "theta = 1.2\n",
    "\n",
    "def prior(x, alpha, theta):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcae7e-493b-4319-a3a3-ef67f71e45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the prior, just for fun\n",
    "xs = np.linspace(0, 40, 500)\n",
    "ys = \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, ys, label='prior')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1129964-4ea6-411f-b348-363377bb6d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the posterior\n",
    "def posterior(lmbd, data):\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c21c5-3385-4bd0-ad98-770c57a1615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot both prior and posterior\n",
    "zs = np.array([posterior(x, asbestos) for x in xs])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, ys, label='prior')\n",
    "plt.plot(xs, zs, label='posterior')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca915f9-7d6c-4a86-b548-b2139574b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the mean of the posterior\n",
    "def MAP(x, data):\n",
    "    return ...\n",
    "\n",
    "map_est = ...\n",
    "\n",
    "print('MAP = ', map_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec5ed7-ec90-4a1c-ae39-065bc056f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random sample from the posterior\n",
    "n = 100 # the sample size\n",
    "sample = np.zeros(n) #the empty sample\n",
    "i = 0 # the counter\n",
    "\n",
    "np.random.seed(123)\n",
    "while i < n:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537934c7-bc8b-4ba3-aa03-eaa5886d622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check visually\n",
    "plt.figure()\n",
    "plt.hist(sample, density=True, edgecolor = 'k', label='the sample')\n",
    "plt.plot(xs, zs, label='the posterior', c='r')\n",
    "plt.legend()\n",
    "plt.xlim(20, 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb0721-02db-4eab-8d97-dae957e6e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the interval\n",
    "m = 1000 # of bootstraps\n",
    "deltas = np.zeros(m)\n",
    "sample_mean = np.mean(sample)\n",
    "\n",
    "np.random.seed(333)\n",
    "for i in range(m):\n",
    "\n",
    "\n",
    "print(f'The 90% \"confidence\" interval is ({l}, {u})\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb866b-7414-4fd5-81eb-6e0bf1713b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
