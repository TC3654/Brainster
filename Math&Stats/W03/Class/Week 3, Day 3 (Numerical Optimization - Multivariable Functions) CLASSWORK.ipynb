{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sophisticated-devon",
   "metadata": {},
   "source": [
    "# Week 3: Calculus (cont'd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hispanic-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# To make 3D graphs \"interactive\" install pympl\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-singer",
   "metadata": {},
   "source": [
    "## Day 3: Numerical Optimization (Multivariable Functions)\n",
    "* Here we continue with the process of fining minimum and maximum points for multivariable functions\n",
    "* Like earlier, we will be using **numerical optimization** methods to find an approximate minima\n",
    "\n",
    "### Gradient Descent Method\n",
    "* We will modify the Gradient Descent Method to work for multivariable functions\n",
    "* Let $f(x, y)$ be a function of two variables $x$ and $y$, and let $f_x(x, y)$ and $f_y(x, y)$ be its partial derivatives.\n",
    "* Starting with some initial approximation $(x_0, y_0)$, we generate a sequence of points $(x_1, y_1), (x_2, y_2), \\ldots$ which approaches a local minimum $(x^*, y^*)$\n",
    "* To obtain the next point in the sequence $(x_{k+1}, y_{k+1})$ from the current point $(x_k, y_k)$ we use the following recursive relations:\n",
    "\\begin{equation}\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "    x_{k+1} &=& x_k - \\alpha\\cdot f_x(x_k, y_k)\\\\\n",
    "    y_{k+1} &=& y_k - \\alpha\\cdot f_y(x_k, y_k)\n",
    "\\end{array}\\right.\n",
    "\\end{equation}\n",
    "where $\\alpha$ is the learning rate of the algorithm.\n",
    "* Alternatively, if we label the points $\\mathbf{x}_k = (x_k, y_k)$, and the gradient $\\nabla f(\\mathbf{x}_k)$, then the recursive relation can be expresed as:\n",
    "\\begin{equation}\n",
    "    \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\cdot \\nabla f(\\mathbf{x}_k)\n",
    "\\end{equation}\n",
    "* The algorithm stops either when the number of iterations $k$ has reached the preset maximum $max\\_iter$, or when the Euclidean norm of the gradient at $\\mathbf{x}_k$ given by $\\|\\nabla f(\\mathbf{x}_k)\\|_2$ becomes less than the preset tolerance $tol$.\n",
    "\n",
    "### Example 1\n",
    "* Find the minimum of the function $f(x, y) = x^2 + y^2 + 1$ using the Gradient Descent Method. Use $(x_0, y_0) = (-5, 7)$ and $\\alpha = 0.1$, with maximum number of allowed iterations $max\\_iter = 500$ and tolerance $tol = 10^{-6}$\n",
    "* This function has only one minimum point $(x^*, y^*) = (0. 0)$ with a minimum value $f(x^*, y^*) = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aware-million",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43da83e0d9c4694b1ed6a05d77f9bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sympy.plotting.plot.Plot at 0x25421d936d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the variables and the function\n",
    "x, y = sp.symbols('x y', real=True)\n",
    "f = sp.Function('f', real=True)\n",
    "f = x**2 + y**2 + 1\n",
    "\n",
    "# Plot the function, just for fun\n",
    "sp.plotting.plot3d(f, (x, -2, 2), (y, -2, 2), size=(7, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "herbal-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x**2 + y**2 + 1\n",
      "2*x\n",
      "2*y\n"
     ]
    }
   ],
   "source": [
    "# Calculating the partial derivatives of f\n",
    "f_x = f.diff(x)\n",
    "f_y = f.diff(y)\n",
    "\n",
    "print(f)\n",
    "print(f_x)\n",
    "print(f_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "applied-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numpy functions for the function and its gradient\n",
    "# x[0] --> x\n",
    "# x[1] --> y\n",
    "def func(x):\n",
    "    return x[0]**2 + x[1]**2 + 1\n",
    "\n",
    "def grad(x):\n",
    "    return np.array( [2*x[0], 2*x[1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "under-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xk =  [-2.69599467e-07  3.77439253e-07]\n",
      "f(xk) =  1.0000000000002152\n",
      "k =  75\n",
      "|grad| =  9.276729216518059e-07\n"
     ]
    }
   ],
   "source": [
    "xk = np.array([-5, 7])\n",
    "alpha = 0.1\n",
    "max_iter = 500\n",
    "tol = 1e-6\n",
    "k = 0\n",
    "\n",
    "while (k < max_iter) and ( np.linalg.norm(grad(xk), 2) >= tol ):\n",
    "    xk = xk - alpha*grad(xk)\n",
    "    k = k + 1\n",
    "\n",
    "print('xk = ', xk)\n",
    "print('f(xk) = ', func(xk))\n",
    "print('k = ', k)\n",
    "print('|grad| = ', np.linalg.norm(grad(xk), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-civilization",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "* Find the minimum of the function $g(x, y) = \\frac{x - y}{x^2 + y^2 + 1}$ using the Gradient Descent Method. Use $(x_0, y_0) = (0.5, -0.5)$ and $\\alpha = 2$, with maximum number of allowed iterations $max\\_iter = 500$ and tolerance $tol = 10^{-6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "looking-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xk =  [-0.7071077  0.7071077]\n",
      "g(xk) =  -0.7071067811859486\n",
      "k =  19\n",
      "|grad| =  9.202362858481398e-07\n"
     ]
    }
   ],
   "source": [
    "x, y = sp.symbols('x[0] x[1]')\n",
    "g = sp.Function('g', real=True)\n",
    "g = (x - y)/(x**2 + y**2 + 1)\n",
    "\n",
    "#print(g)\n",
    "#print(g.diff(x))\n",
    "#print(g.diff(y))\n",
    "\n",
    "def g(x):\n",
    "    return (x[0] - x[1])/(x[0]**2 + x[1]**2 + 1)\n",
    "\n",
    "def grad_g(x):\n",
    "    return np.array( [-2*x[0]*(x[0] - x[1])/(x[0]**2 + x[1]**2 + 1)**2 + 1/(x[0]**2 + x[1]**2 + 1), -2*x[1]*(x[0] - x[1])/(x[0]**2 + x[1]**2 + 1)**2 - 1/(x[0]**2 + x[1]**2 + 1) ] )\n",
    "\n",
    "\n",
    "xk = np.array([0.5, -0.5])\n",
    "alpha = 2\n",
    "max_iter = 500\n",
    "tol = 1e-6\n",
    "k = 0\n",
    "\n",
    "while (k < max_iter) and ( np.linalg.norm(grad_g(xk), 2) >= tol ):\n",
    "    xk = xk - alpha*grad_g(xk)\n",
    "    k = k + 1\n",
    "\n",
    "print('xk = ', xk)\n",
    "print('g(xk) = ', g(xk))\n",
    "print('k = ', k)\n",
    "print('|grad| = ', np.linalg.norm(grad_g(xk), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7304afcd-2ec7-4c6d-95d4-bec800139ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493500acbd0945ce80583dc8553ad5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<sympy.plotting.plot.Plot at 0x25424d9b8e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.plotting.plot3d(g, (x, -3 ,3), (y, -3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-sodium",
   "metadata": {},
   "source": [
    "### Example 2a\n",
    "* Under the same conditions, find the maximum point of the function $g(x, y)$ from **Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cathedral-baking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xk =  [ 0.70710749 -0.70710749]\n",
      "g(xk) =  0.7071067811861912\n",
      "k =  14\n",
      "|grad| =  7.097290933435598e-07\n"
     ]
    }
   ],
   "source": [
    "xk = np.array([0.5, -0.5])\n",
    "alpha = 2\n",
    "max_iter = 500\n",
    "tol = 1e-6\n",
    "k = 0\n",
    "\n",
    "while (k < max_iter) and ( np.linalg.norm(grad_g(xk), 2) >= tol ):\n",
    "    xk = xk + alpha*grad_g(xk)\n",
    "    k = k + 1\n",
    "\n",
    "print('xk = ', xk)\n",
    "print('g(xk) = ', g(xk))\n",
    "print('k = ', k)\n",
    "print('|grad| = ', np.linalg.norm(grad_g(xk), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-seller",
   "metadata": {},
   "source": [
    "### Putting the all of it together in a single function\n",
    "* Write a Python function that will be an implementation of the Gradient Descent Method\n",
    "* Input arguments are:\n",
    "    * The function $f$ and its gradient $grad$\n",
    "    * Initial approximation $x_0$\n",
    "    * Learning rate *alpha*, by default set to 0.01\n",
    "    * Maximum number of iterations *max_iter*, by default set to 1000\n",
    "    * Tolerance level for the gradient *tol*, by default set to 1e-6\n",
    "* Output arguments are:\n",
    "    * The minimum $x^*$\n",
    "    * The minimum value $f(x^*)$\n",
    "    * The Euclidean norm of the gradient at the minimum, $\\left\\|\\nabla f(x^*)\\right\\|$\n",
    "    * The number of iterations it took to obtain the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "female-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad, x0, alpha = 0.01, mode = 'min', max_iter = 1000, tol = 1e-6):\n",
    "    # initialize the sequence\n",
    "    k = 0\n",
    "    xk = x0\n",
    "    grad_k = grad(x0)\n",
    "    \n",
    "    while (k < max_iter) and (np.linalg.norm(grad_k, 2) >= tol):\n",
    "        k = k + 1\n",
    "        if mode == 'min':\n",
    "            xk = xk - alpha * grad_k\n",
    "        else:\n",
    "            xk = xk + alpha * grad_k\n",
    "        grad_k = grad(xk)\n",
    "    \n",
    "    print('xk = ', xk)\n",
    "    print('func(xk) = ', f(xk))\n",
    "    print('|grad| = ', np.linalg.norm(grad_k, 2))\n",
    "    print('k = ', k)\n",
    "    \n",
    "    return xk, f(xk), np.linalg.norm(grad_k, 2), k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "available-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xk =  [-0.7071077  0.7071077]\n",
      "func(xk) =  -0.7071067811859486\n",
      "|grad| =  9.202362858481398e-07\n",
      "k =  19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.7071077,  0.7071077]),\n",
       " -0.7071067811859486,\n",
       " 9.202362858481398e-07,\n",
       " 19)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test if the function works ok\n",
    "x0 = np.array([0.5, -0.5])\n",
    "gradient_descent(g, grad_g, x0, alpha=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-atlantic",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "* Find the minimum of the function $f(\\mathbf{x}) = (x_1 - 2)^4 + (x_1 - 2)^2 \\cdot x_2^2 + (x_2 + 1)^2$ where $\\mathbf{x} = (x_1, x_2)$ starting with an initial approximation $\\mathbf{x}_0 = (1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables and function\n",
    "x1, x2 = sp.symbols('x[0] x[1]', real=True)\n",
    "f = sp.Function('f', real=True)\n",
    "f = (x1 - 2)**4 + (x1 - 2)**2 * x2**2 + (x2 + 1)**2\n",
    "\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return \n",
    "\n",
    "def grad(x):\n",
    "    return \n",
    "\n",
    "x0 = np.array([1, 1])\n",
    "gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-bargain",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "* Find the minimum of the function $f(\\mathbf{x}) = (1 - x_1)^2 + (2 - x_2)^2 + (3 - x_3)^2 + 4(1 - x_4)^2$ using the gradient descent method and an initial approximation $\\mathbf{x}_0 = (-2, -1, 1, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "numerical-relay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 - x[0])**2 + 4*(1 - x[3])**2 + (2 - x[1])**2 + (3 - x[2])**2\n",
      "2*x[0] - 2\n",
      "2*x[1] - 4\n",
      "2*x[2] - 6\n",
      "8*x[3] - 8\n"
     ]
    }
   ],
   "source": [
    "# Define variables and function\n",
    "x1, x2, x3, x4 = sp.symbols('x[0] x[1] x[2] x[3]', real=True)\n",
    "f = sp.Function('f', real=True)\n",
    "f = (1 - x1)**2 + (2 - x2)**2 + (3 - x3)**2 + 4*(1 - x4)**2\n",
    "\n",
    "#print the function and the partial derivatives\n",
    "print(f)\n",
    "print(f.diff(x1))\n",
    "print(f.diff(x2))\n",
    "print(f.diff(x3))\n",
    "print(f.diff(x4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "382ee117-e85f-4a89-945c-29aeb89cc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (1 - x[0])**2 + 4*(1 - x[3])**2 + (2 - x[1])**2 + (3 - x[2])**2\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([\n",
    "        2*x[0] - 2,\n",
    "        2*x[1] - 4,\n",
    "        2*x[2] - 6,\n",
    "        8*x[3] - 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "120650ca-d36c-47d0-8c71-a8d8be2daf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xk =  [0.99999968 1.99999968 2.99999979 1.        ]\n",
      "func(xk) =  2.465375251869951e-13\n",
      "|grad| =  9.93050905416223e-07\n",
      "k =  795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.99999968, 1.99999968, 2.99999979, 1.        ]),\n",
       " 2.465375251869951e-13,\n",
       " 9.93050905416223e-07,\n",
       " 795)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min = (1, 2, 3 1)\n",
    "x0 = np.array([-2, -1, 1, 2])\n",
    "gradient_descent(f, grad, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-cocktail",
   "metadata": {},
   "source": [
    "### Example 5: Rosenbrock's Function\n",
    "* Find the minimum of the function $r(x, y) = 100(y - x^2)^2 + (1 - x)^2$ using the Gradient Descent Method. Use $(x_0, y_0) = (-1.2, 1.0)$, with maximum number of allowed iterations $max\\_iter = 10000$ and tolerance $tol = 10^{-6}$. You will have to tune the value of the learning rate $\\alpha$ by trial-and-error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "characteristic-bracelet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154fbce831624db081c602ccda00eafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 - x[0])**2 + 100*(-x[0]**2 + x[1])**2\n",
      "-400*x[0]*(-x[0]**2 + x[1]) + 2*x[0] - 2\n",
      "-200*x[0]**2 + 200*x[1]\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock's Function\n",
    "x, y = sp.symbols('x[0] x[1]')\n",
    "ros = sp.Function('f', real=True)\n",
    "ros = 100*(y - x**2)**2 + (1 - x)**2\n",
    "\n",
    "\n",
    "\n",
    "sp.plotting.plot3d(ros, (x, -1.5, 1.5), (y, -1, 3))\n",
    "\n",
    "print(ros)\n",
    "print(ros.diff(x))\n",
    "print(ros.diff(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da439c18-f890-46af-8d86-14fd6a2d23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(x):\n",
    "    return (1 - x[0])**2 + 100*(-x[0]**2 + x[1])**2\n",
    "\n",
    "def grad_r(x):\n",
    "    return np.array([\n",
    "        -400*x[0]*(-x[0]**2 + x[1]) + 2*x[0] - 2,\n",
    "        -200*x[0]**2 + 200*x[1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f16cca0a-e4d8-4446-94c6-63f6251ce7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xk =  [0.44519069 0.12051672]\n",
      "func(xk) =  0.9112009629940887\n",
      "|grad| =  20.080578712399298\n",
      "k =  100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.44519069, 0.12051672]),\n",
       " 0.9112009629940887,\n",
       " 20.080578712399298,\n",
       " 100000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min = (1, 1)\n",
    "x0 = np.array([-1.2, 1.0])\n",
    "gradient_descent(r, grad_r, x0, alpha = 0.006, max_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d6e9170-9ecc-4d06-bd29-f3ddc7c2f6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 2.5352978256684153e-15\n",
      " hess_inv: array([[0.50186009, 1.00315326],\n",
      "       [1.00315326, 2.01027466]])\n",
      "      jac: array([-1.70451176e-06,  8.23257352e-07])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 39\n",
      "      nit: 32\n",
      "     njev: 39\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0.99999997, 0.99999995])\n"
     ]
    }
   ],
   "source": [
    "out = opt.minimize(r, x0, jac=grad_r)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-advocacy",
   "metadata": {},
   "source": [
    "### Example 6: solving systems of equations using optimization\n",
    "* A system of equations given by $A\\mathbf{x} = b$ where $A$ is a matrix, $b$ is a vector, and $\\mathbf{x}$ is the solution can be solved using optimization tools\n",
    "* The approach takes advandage of the fact that any norm is either positive or zero, and that the norm of the zero-vector $\\mathbf{O}$ is always zero. It works like this: start with the system\n",
    "\\begin{equation}\n",
    "A\\mathbf{x} = b\n",
    "\\end{equation}\n",
    "Rewrite:\n",
    "\\begin{equation}\n",
    "A\\mathbf{x} - b = \\mathbf{O}\n",
    "\\end{equation}\n",
    "Calculate Euclidean norm on both sides of the equation:\n",
    "\\begin{equation}\n",
    "\\left\\| A\\mathbf{x} - b \\right\\| = \\|\\mathbf{O}\\|\n",
    "\\end{equation}\n",
    "Since $\\|\\mathbf{O}\\| = 0$, then:\n",
    "\\begin{equation}\n",
    "\\left\\| A\\mathbf{x} - b \\right\\| = 0\n",
    "\\end{equation}\n",
    "To make life easier, we want to remove the square root from the Euclidean norm on the left-hand side, so we square both sides:\n",
    "\\begin{equation}\n",
    "\\left\\| A\\mathbf{x} - b \\right\\|^2 = 0\n",
    "\\end{equation}\n",
    "Bottom line: if $\\mathbf{x}$ is solution to the system $A\\mathbf{x} = b$, then $\\mathbf{x}$ is minimum of the function $f(\\mathbf{x}) = \\left\\| A\\mathbf{x} - b \\right\\|^2$. Thus, to solve a system of equations, the only thing we need to do is define the function $f$ and minimize it.\n",
    "\n",
    "* Solve the system:\n",
    "\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "x + y + z &=& 6\\\\\n",
    "2x - y + z &=& 3\\\\\n",
    "3x + y - z &=& 2\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "missing-gamma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(x[0] + x[1] + x[2] - 6)**2 + (2*x[0] - x[1] + x[2] - 3)**2 + (3*x[0] + x[1] - x[2] - 2)**2\n",
      "28*x[0] + 4*x[1] - 36\n",
      "4*x[0] + 6*x[1] - 2*x[2] - 10\n",
      "-2*x[1] + 6*x[2] - 14\n"
     ]
    }
   ],
   "source": [
    "# Define varibales and matrices\n",
    "A = sp.Matrix([[1, 1, 1], [2, -1, 1], [3, 1, -1]])\n",
    "b = sp.Matrix([6, 3, 2])\n",
    "x, y, z, X = sp.symbols('x[0] x[1] x[2] X', real=True)\n",
    "X = sp.Matrix([x, y, z])\n",
    "\n",
    "# Define the function f\n",
    "f = sp.Function('f', real=True)\n",
    "f = ((A*X - b).norm())**2\n",
    "\n",
    "print(f)\n",
    "print(f.diff(x))\n",
    "print(f.diff(y))\n",
    "print(f.diff(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "sudden-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numpy function f\n",
    "def f(x):\n",
    "    return (x[0] + x[1] + x[2] - 6)**2 + (2*x[0] - x[1] + x[2] - 3)**2 + (3*x[0] + x[1] - x[2] - 2)**2\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([\n",
    "        28*x[0] + 4*x[1] - 36,\n",
    "        4*x[0] + 6*x[1] - 2*x[2] - 10,\n",
    "        -2*x[1] + 6*x[2] - 14\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf804e86-0d9a-4c82-bd35-61d2a5bfb383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xk =  [1.00000003 1.9999998  2.99999983]\n",
      "func(xk) =  1.3145685663806098e-13\n",
      "|grad| =  9.789028750125397e-07\n",
      "k =  446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.00000003, 1.9999998 , 2.99999983]),\n",
       " 1.3145685663806098e-13,\n",
       " 9.789028750125397e-07,\n",
       " 446)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([6, -1, 1])\n",
    "gradient_descent(f, grad, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eb826ce3-0593-4691-bc10-c32e711107f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 0.0\n",
       " hess_inv: array([[ 0.04, -0.03, -0.01],\n",
       "       [-0.03,  0.21,  0.07],\n",
       "       [-0.01,  0.07,  0.19]])\n",
       "      jac: array([0., 0., 0.])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 9\n",
       "      nit: 6\n",
       "     njev: 9\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([1., 2., 3.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.minimize(f, x0, jac=grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b02fa5-89f6-465a-a5f7-6485af30749d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
